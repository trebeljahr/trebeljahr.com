---
title: "100k Token Contexts, AI Tutors and Google/IO"
cover:
    src: "/assets/midjourney/majestic-waterfall-cascading-into-a-clear-turquoise-pond.jpg"
    alt: "majestic waterfall cascading into a clear turquoise pond"
    width: 780
    height: 780
excerpt: "Welcome to this edition of Live and Learn. This time it's one of those editions, where the amount of things that have happened recently is simply incomprehensible. I cut out so much that I would have liked to include‚Ä¶ And still, this edition grew beyond the 4-5 links that I try to set myself as a soft limit. AI is moving at an ever-accelerating pace and this becomes more and more evident the more I read about it. There are way over 100 tabs bookmarked that I haven‚Äôt read yet, and soon I will bookmark the next 100. And this to me is insane. It feels great to dig into all that is happening but at the same time, it‚Äôs overwhelming. Anyways. This edition contains some insights from the GoogleIO, a great article about how open source is taking over LLTM research by storm, and an exploration of the music generation capabilities of ChatGPT. As always I hope you find something useful and enjoy this newsletter."
tags: ["AI", "progress", "future", ‚ÄúLLTM‚Äù, ‚ÄúOpenAI‚Äù, ‚ÄúGoogle Keynote‚Äù, ‚ÄúGoogle‚Äù, ‚ÄúVertex AI‚Äù, ‚ÄúAnthropic‚Äù, ‚ÄúGPT-4‚Äù, ‚ÄúAI tutors‚Äù, ‚Äúgenerative AI‚Äù]
---

## ‚ú® Quote ‚ú®

> In perhaps nature‚Äôs most miraculous transformation, the universe evolved the capacity to ponder and understand itself. 

‚Äî Edward Kolb

## üñáÔ∏è Links üñáÔ∏è

[**GPT-4 explains GPT-2 neurons** by OpenAI](https://openai.com/research/language-models-can-explain-neurons-in-language-models). As models get more powerful we can turn them back on themselves to help us understand what they do. Here they prompted GPT-4 to simulate how neurons in GPT-2 work. And then scored those simulations and found short descriptions that encapsulate what activates those neurons. This leads to a high-level understanding of *what* the neurons are doing. By using this technique they can ‚Äúlabel‚Äù most of the neurons within GPT-2. This is already impressive as is. But their goal is to explain *everything* that their models do, end to end. In the words of OpenAI: ‚ÄúWe want to eventually automatically find and explain entire neural circuits implementing complex behaviors.‚Äù Let‚Äôs see how far they get. 

[**Introducing 100k Context Windows** by Anthropic](https://www.anthropic.com/index/100k-context-windows). You can now officially feed LLTMs entire books in a single prompt. This blows my mind. I mean seriously, 100k in tokens equals roughly 75k words, which is the average length of most novels out there. Just imagine that you knew somebody who could read and summarize, and to a certain extent *understand* all of ‚ÄúHarry Potter and the Philosopher's Stone‚Äù (76944 words) in a little less than a minute. Now stop imagining it, because well, we have the technology to do that. 

[**We have No Moat** by SemiAnalysis](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). This leaked Google insider document, argues that open source is starting to dominate over OpenAI‚Äôs and Google's efforts. And that these companies should embrace the open-source culture, instead of fighting it. The examples this piece links to alone, make it worth reading but the analysis is really where it shines. Because the article builds a compelling case as to why open source changes the research landscape. And then goes on to explain why this change is something that Google and other companies like it, should worry about. There is a timeline at the end that shows, just how fast things evolve once the open-source community gets involved. And instead of fearing that competition, Google should embrace and try to use that power to its advantage.

[**Google I/O Keynote** by Google](https://youtu.be/cNfINi5CNbY). I've been watching the Google Keynote sessions for quite some years now and they never ceased to amaze me. However, with the current rate of progress in AI development, this keynote felt a bit lackluster. It's weird to see Google playing catch-up with OpenAI and see them trying to convince the world that they are still a powerhouse within AI research and operations. The coolest announcement, in my opinion, was that of [Vertex AI](https://cloud.google.com/blog/products/ai-machine-learning/google-cloud-launches-vertex-ai-unified-platform-for-mlops?hl=en). A unified platform for doing machine learning training, fine-tuning, deployment and all the other things that engineers will need to build AI into their products. Google is positioning itself to be not just a model provider, but an infrastructure powerhouse. They want to sell an out-of-the-box, easy-to-use platform so that everybody can integrate AI into their business. I think this is a great idea and I am looking forward to seeing where this is going. But I am also hyped as to what AWS, Microsoft, and Nvidia are going to do in this space as well.  

[**Mr. Ranedeer** by JushBJJ](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor). One of the coolest personalized AI tutors I have seen. With it, you can learn pretty much anything, really quickly. This prompt feels like magic. It shows how people can configure large language models to solve very specific tasks. This is what "prompt engineering" really means. Copy-pasting the prompt and being greeted exactly with what the prompt was *engineered* to do. An achievement like that is simply and utterly mindblowing. If you want to learn *anything* I think tutors like Mr. Ranedeer are one of the best ways out there today. Khan Academy also rolled out their [Khanmigo](https://www.khanacademy.org/khan-labs)  tutor a while back.

[**But can ChatGPT-4 write a good melody?** by Marc Evanstein](https://www.youtube.com/watch?v=d_7EsKcn8nw). This is an exploration of using GPT-4 to create music. Right now GPT-4 is not great at this task, but still it can produce small coherent musical phrases. But if you stop for a second and think about what it is actually doing it is already *extremely* good. It‚Äôs only a large *language* model after all, and yet it can generate melodies! How insane is that? Just keep in mind that it has never learned the rules of music. Nor does it hear or have a concept of sound as such... I mean, seriously, it can only output Midi notes as Python code. Try to imagine somebody who has never heard music in their life and was told to compose a piece and who then goes on to do a pretty decent job at it... That would be freaking impressive. And that is *exactly* what ChatGPT is doing in this video. Which is absolutely remarkable.  

## üåå Midjourney üåå

![person exploring a volcano on an alien planet](/assets/midjourney/person-exploring-a-volcano-on-an-alien-planet.jpg)
![a photorealistic image of a rugged cliff with a beautiful woman in a red dress standing on top](/assets/midjourney/a-photorealistic-image-of-a-rugged-cliff-with-a-beautiful-woman-in-a-red-dress-standing-on-top.jpg)
![an explorer studying the wildlife on a marine exoplanet](/assets/midjourney/an-explorer-studying-the-wildlife-on-a-marine-exoplanet.jpg)
![a hiker exploring a field of flowers in the mountains](/assets/midjourney/a-hiker-exploring-a-field-of-flowers-in-the-mountains.jpg)



## üé∂ Song üé∂

**Moonlight Sonata, 3rd Movement** by Beethoven

[Youtube Music](https://music.youtube.com/watch?v=BV7RkEL6oRc) | [Spotify](https://open.spotify.com/track/6jBT9MBVjX4kZ68IV6wHnH) 

---

I hope you found this newsletter useful, beautiful, or even both!

Have ideas for improving it? [Please let me know](https://airtable.com/shro1VeyG4lkNXkx2). 

Cheers,

**‚Äì Rico**
